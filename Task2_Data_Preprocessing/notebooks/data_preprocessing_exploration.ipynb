{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing Exploration\n",
        "\n",
        "This notebook provides an interactive exploration of the data preprocessing steps performed in the project.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Loading the Dataset](#loading-the-dataset)\n",
        "2. [Exploratory Data Analysis](#exploratory-data-analysis)\n",
        "3. [Handling Missing Values](#handling-missing-values)\n",
        "4. [Detecting and Handling Outliers](#detecting-and-handling-outliers)\n",
        "5. [Feature Scaling](#feature-scaling)\n",
        "6. [Data Splitting](#data-splitting)\n",
        "7. [Visualization](#visualization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context(\"notebook\", font_scale=1.2)\n",
        "colors = sns.color_palette(\"viridis\", 8)\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 20)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Loading the Dataset <a id=\"loading-the-dataset\"></a>\n",
        "\n",
        "Let's load the dataset and take a look at its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('../data/raw/CVD_cleaned.csv')\n",
        "\n",
        "# Display basic information\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check data types\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis <a id=\"exploratory-data-analysis\"></a>\n",
        "\n",
        "Let's explore the dataset to understand its characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percent = (missing_values / len(df)) * 100\n",
        "missing_data = pd.concat([missing_values, missing_percent], axis=1)\n",
        "missing_data.columns = ['Count', 'Percent']\n",
        "missing_data[missing_data['Count'] > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize distributions of numerical features\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    sns.histplot(df[col], kde=True, color=colors[i])\n",
        "    plt.title(f'Distribution of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize boxplots to identify outliers\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    sns.boxplot(y=df[col], color=colors[i])\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation_matrix = df[numerical_cols].corr()\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "plt.title('Correlation Matrix', fontsize=18)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Handling Missing Values <a id=\"handling-missing-values\"></a>\n",
        "\n",
        "Let's handle any missing values in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to handle missing values\n",
        "def handle_missing_values(df, strategy='median'):\n",
        "    # Create a copy of the dataframe\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # Handle numerical and categorical features separately\n",
        "    numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    categorical_features = df.select_dtypes(include=['object']).columns\n",
        "    \n",
        "    # Impute numerical features\n",
        "    if len(numerical_features) > 0:\n",
        "        imputer = SimpleImputer(strategy=strategy)\n",
        "        df_processed[numerical_features] = imputer.fit_transform(df[numerical_features])\n",
        "    \n",
        "    # Impute categorical features with most frequent value\n",
        "    if len(categorical_features) > 0:\n",
        "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "        df_processed[categorical_features] = cat_imputer.fit_transform(df[categorical_features])\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "# Apply missing value handling\n",
        "df_no_missing = handle_missing_values(df, strategy='median')\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(f\"Missing values after imputation: {df_no_missing.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Detecting and Handling Outliers <a id=\"detecting-and-handling-outliers\"></a>\n",
        "\n",
        "Let's detect and handle outliers in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to handle outliers using IQR method\n",
        "def handle_outliers_iqr(df):\n",
        "    # Create a copy of the dataframe\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # Get numerical features\n",
        "    numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    \n",
        "    # IQR method for outlier detection\n",
        "    outliers_summary = {}\n",
        "    for column in numerical_features:\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        \n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        \n",
        "        # Count outliers\n",
        "        outliers_count = ((df[column] < lower_bound) | (df[column] > upper_bound)).sum()\n",
        "        outliers_summary[column] = outliers_count\n",
        "        \n",
        "        # Replace outliers with bounds\n",
        "        df_processed[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
        "        df_processed[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
        "    \n",
        "    return df_processed, outliers_summary\n",
        "\n",
        "# Apply outlier handling\n",
        "df_no_outliers, outliers_summary = handle_outliers_iqr(df_no_missing)\n",
        "\n",
        "# Display outliers summary\n",
        "pd.Series(outliers_summary).sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare boxplots before and after outlier handling\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    plt.subplot(2, len(numerical_cols), i+1)\n",
        "    sns.boxplot(y=df_no_missing[col], color=colors[i])\n",
        "    plt.title(f'{col} (Before)')\n",
        "    \n",
        "    plt.subplot(2, len(numerical_cols), i+1+len(numerical_cols))\n",
        "    sns.boxplot(y=df_no_outliers[col], color=colors[i])\n",
        "    plt.title(f'{col} (After)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Scaling <a id=\"feature-scaling\"></a>\n",
        "\n",
        "Let's scale the features to have a standard range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to scale features\n",
        "def scale_features(df, method='standard'):\n",
        "    # Create a copy of the dataframe\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # Get numerical features\n",
        "    numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    \n",
        "    # Initialize the appropriate scaler\n",
        "    if method == 'standard':\n",
        "        scaler = StandardScaler()\n",
        "    elif method == 'minmax':\n",
        "        scaler = MinMaxScaler()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported scaling method. Use 'standard' or 'minmax'.\")\n",
        "    \n",
        "    # Fit and transform the numerical features\n",
        "    df_processed[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "    \n",
        "    return df_processed, scaler\n",
        "\n",
        "# Apply feature scaling\n",
        "df_scaled, scaler = scale_features(df_no_outliers, method='standard')\n",
        "\n",
        "# Display scaled data\n",
        "df_scaled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare distributions before and after scaling\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    plt.subplot(2, len(numerical_cols), i+1)\n",
        "    sns.histplot(df_no_outliers[col], kde=True, color=colors[i])\n",
        "    plt.title(f'{col} (Before Scaling)')\n",
        "    \n",
        "    plt.subplot(2, len(numerical_cols), i+1+len(numerical_cols))\n",
        "    sns.histplot(df_scaled[col], kde=True, color=colors[i])\n",
        "    plt.title(f'{col} (After Scaling)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Splitting <a id=\"data-splitting\"></a>\n",
        "\n",
        "Let's split the data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify the target column for cardiovascular disease prediction\n",
        "target_column = \"Heart_Disease\"\n",
        "print(f\"Target column: {target_column}\")\n",
        "\n",
        "# Split the data\n",
        "X = df_scaled.drop(target_column, axis=1)\n",
        "y = df_scaled[target_column]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization <a id=\"visualization\"></a>\n",
        "\n",
        "Let's visualize the processed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCA visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_train)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap='viridis', alpha=0.8, edgecolors='w')\n",
        "plt.colorbar(scatter, label='Target Variable')\n",
        "plt.title('PCA: 2D Projection of the Dataset', fontsize=18)\n",
        "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance using Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Create feature importance plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(range(X_train.shape[1]), importances[indices], align='center', color=colors)\n",
        "plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
        "plt.title('Feature Importance from Random Forest', fontsize=18)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've performed a comprehensive data preprocessing pipeline:\n",
        "\n",
        "1. Loaded and explored the dataset\n",
        "2. Handled missing values using median imputation\n",
        "3. Detected and handled outliers using the IQR method\n",
        "4. Scaled features using standardization\n",
        "5. Split the data into training and testing sets\n",
        "6. Visualized the processed data using PCA and feature importance\n",
        "\n",
        "The preprocessed data is now ready for model training and evaluation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
